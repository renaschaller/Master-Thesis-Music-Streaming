{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c2e042d",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd40f256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users kept: 14421 | Sequence tensor: (14421, 60, 62) (N, T, D)\n",
      "Split shapes: (10094, 60, 62) (2163, 60, 62) (2164, 60, 62)\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " seq (InputLayer)            [(None, 60, 62)]          0         \n",
      "                                                                 \n",
      " masking (Masking)           (None, 60, 62)            0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                32512     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36737 (143.50 KB)\n",
      "Trainable params: 36737 (143.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "79/79 [==============================] - 27s 218ms/step - loss: 0.9493 - AUC: 0.5758 - val_loss: 0.7018 - val_AUC: 0.6307 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 0.9179 - AUC: 0.6401 - val_loss: 0.6974 - val_AUC: 0.6380 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "79/79 [==============================] - 15s 192ms/step - loss: 0.9112 - AUC: 0.6512 - val_loss: 0.6721 - val_AUC: 0.6399 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "79/79 [==============================] - 13s 166ms/step - loss: 0.9046 - AUC: 0.6619 - val_loss: 0.7170 - val_AUC: 0.6419 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "79/79 [==============================] - 13s 166ms/step - loss: 0.8963 - AUC: 0.6723 - val_loss: 0.6963 - val_AUC: 0.6535 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "79/79 [==============================] - 15s 193ms/step - loss: 0.8953 - AUC: 0.6738 - val_loss: 0.7207 - val_AUC: 0.6511 - lr: 0.0010\n",
      "Epoch 7/15\n",
      "79/79 [==============================] - 13s 163ms/step - loss: 0.8917 - AUC: 0.6782 - val_loss: 0.6801 - val_AUC: 0.6503 - lr: 0.0010\n",
      "Epoch 8/15\n",
      "79/79 [==============================] - 13s 165ms/step - loss: 0.8867 - AUC: 0.6844 - val_loss: 0.6951 - val_AUC: 0.6491 - lr: 5.0000e-04\n",
      "17/17 [==============================] - 3s 69ms/step\n",
      "17/17 [==============================] - 1s 79ms/step\n",
      "Validation AUC: 0.6536\n",
      "Test AUC:       0.6488\n",
      "Chosen threshold (Youden J on val): 0.60\n",
      "\n",
      "Test classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.67      0.67      1269\n",
      "           1       0.54      0.55      0.55       895\n",
      "\n",
      "    accuracy                           0.62      2164\n",
      "   macro avg       0.61      0.61      0.61      2164\n",
      "weighted avg       0.62      0.62      0.62      2164\n",
      "\n",
      "Confusion matrix (test):\n",
      "[[844 425]\n",
      " [399 496]]\n"
     ]
    }
   ],
   "source": [
    "# %% ================================\n",
    "# Recurrent model for user activity (LSTM, one-hot cats, no IDs)\n",
    "# ================================\n",
    "# Config\n",
    "DATA_PATH = \"../data/cleaned_data.csv\"\n",
    "\n",
    "# Sequence construction\n",
    "MAX_STEPS_PER_USER = 60       # first N impressions as \"early actions\"\n",
    "MIN_STEPS_REQUIRED  = 5       # drop users with too few rows\n",
    "ORDER_BY = [\"day_dt\", \"impressPosition_log\"]  # use parsed datetime for stable temporal order\n",
    "\n",
    "# Train/val/test split (by user)\n",
    "VAL_SIZE   = 0.15\n",
    "TEST_SIZE  = 0.15\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Model / training\n",
    "LSTM_UNITS   = 64\n",
    "DROPOUT      = 0.2\n",
    "BATCH_SIZE   = 128\n",
    "EPOCHS       = 15\n",
    "LEARNING_RATE = 1e-3\n",
    "POS_CLASS_WEIGHT = 2.0   # upweight positives if imbalanced\n",
    "\n",
    "# %% ----------------\n",
    "# Imports\n",
    "# -------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "import sklearn\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.keras.utils.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "# %% ----------------\n",
    "# Load data\n",
    "# -------------------\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Optional: handle common column typos (uncomment if needed)\n",
    "# if \"PublishMlogCnt\" in df.columns and \"PushlishMlogCnt\" not in df.columns:\n",
    "#     df = df.rename(columns={\"PublishMlogCnt\": \"PushlishMlogCnt\"})\n",
    "\n",
    "# %% ----------------\n",
    "# Column specification\n",
    "# -------------------\n",
    "# Binary early actions\n",
    "binary_actions = [\n",
    "    \"isClick\",\"isComment\",\"isIntoPersonalHomepage\",\"isShare\",\"isViewComment\",\"isLike\"\n",
    "]\n",
    "\n",
    "# Continuous/log features (ensure numeric)\n",
    "cont_feats = [\"mlogViewTime_log\",\"impressPosition_log\",\"followCnt_log\",\"pop_index_pca_lag1\"]\n",
    "\n",
    "# Categorical (one-hot)\n",
    "cat_feats = [\"province\",\"type\",\"creatorType\"]\n",
    "\n",
    "# Numeric context (we'll use a numeric day_index, not raw day string)\n",
    "num_feats = [\"creator_level\",\"PushlishMlogCnt\",\"age_gender_missing\",\"day_index\"]\n",
    "\n",
    "# Target + grouping key\n",
    "TARGET   = \"y_active\"\n",
    "USER_KEY = \"userId\"\n",
    "\n",
    "# %% ----------------\n",
    "# Parse day as datetime and build numeric day_index\n",
    "# -------------------\n",
    "# If CSV's 'day' is string like '2019-11-23', parse it\n",
    "if \"day\" in df.columns:\n",
    "    df[\"day_dt\"] = pd.to_datetime(df[\"day\"], errors=\"coerce\")\n",
    "else:\n",
    "    raise KeyError(\"Expected column 'day' not found in the CSV.\")\n",
    "\n",
    "# Build day_index = days since min(date)\n",
    "min_day = df[\"day_dt\"].min()\n",
    "df[\"day_index\"] = (df[\"day_dt\"] - min_day).dt.days.astype(\"float32\")  # keep as float for model\n",
    "\n",
    "# %% ----------------\n",
    "# Ensure numeric columns are numeric (coerce strings; keep NaN where needed)\n",
    "# -------------------\n",
    "for c in binary_actions + cont_feats + [\"creator_level\",\"PushlishMlogCnt\",\"age_gender_missing\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# Categorical columns: make strings and add explicit missing token\n",
    "for c in cat_feats:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype(\"string\").fillna(\"__MISSING__\")\n",
    "    else:\n",
    "        raise KeyError(f\"Missing categorical column: {c}\")\n",
    "\n",
    "# Ensure required columns exist\n",
    "required = set(binary_actions + cont_feats + cat_feats + num_feats + [TARGET, USER_KEY, \"day_dt\"])\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing expected columns: {missing}\")\n",
    "\n",
    "# Build keep list (include ORDER_BY columns); remove duplicates preserving order\n",
    "keep_cols = list(dict.fromkeys(binary_actions + cont_feats + cat_feats + num_feats + [USER_KEY, TARGET] + ORDER_BY))\n",
    "df = df[keep_cols].copy()\n",
    "\n",
    "# Sort deterministically within user\n",
    "df = df.sort_values([USER_KEY] + ORDER_BY, kind=\"mergesort\")\n",
    "\n",
    "# y_active should be user-level; if multiple rows per user, take max (ever-active)\n",
    "user_target = df.groupby(USER_KEY)[TARGET].max().astype(int)\n",
    "\n",
    "# %% ----------------\n",
    "# One-hot encoder (version-safe)\n",
    "# -------------------\n",
    "skver = tuple(map(int, sklearn.__version__.split(\".\")[:2]))\n",
    "if skver >= (1, 4):\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "else:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "\n",
    "# Fit encoder on all rows (categoricals only)\n",
    "ohe.fit(df[cat_feats])\n",
    "\n",
    "# Transform helper: OHE cats + stack numeric features\n",
    "def transform_rows(frame: pd.DataFrame) -> np.ndarray:\n",
    "    X_cat = ohe.transform(frame[cat_feats])          # sparse\n",
    "    X_cat = X_cat.toarray().astype(np.float32)       # densify\n",
    "    X_num = frame[binary_actions + cont_feats + num_feats].to_numpy(dtype=np.float32)  # includes NaNs fine\n",
    "    return np.hstack([X_cat, X_num]).astype(np.float32)\n",
    "\n",
    "# Row feature dimension\n",
    "_row_sample = transform_rows(df.iloc[[0]])\n",
    "ROW_FEAT_DIM = _row_sample.shape[1]\n",
    "\n",
    "# %% ----------------\n",
    "# Build user-level sequences\n",
    "# -------------------\n",
    "Xs, ys, user_list, lengths = [], [], [], []\n",
    "for uid, g in df.groupby(USER_KEY, sort=False):\n",
    "    n = len(g)\n",
    "    if n < MIN_STEPS_REQUIRED:\n",
    "        continue\n",
    "    g_feat = transform_rows(g)\n",
    "    g_feat = g_feat[:MAX_STEPS_PER_USER]\n",
    "    if g_feat.shape[0] < MAX_STEPS_PER_USER:\n",
    "        pad_len = MAX_STEPS_PER_USER - g_feat.shape[0]\n",
    "        g_feat = np.vstack([g_feat, np.zeros((pad_len, ROW_FEAT_DIM), dtype=np.float32)])\n",
    "    Xs.append(g_feat)\n",
    "    ys.append(int(user_target.loc[uid]))\n",
    "    user_list.append(uid)\n",
    "    lengths.append(min(n, MAX_STEPS_PER_USER))\n",
    "\n",
    "X = np.stack(Xs, axis=0)  # [N_users, T, D]\n",
    "y = np.array(ys, dtype=np.int32)\n",
    "lengths = np.array(lengths, dtype=np.int32)\n",
    "print(f\"Users kept: {len(user_list)} | Sequence tensor: {X.shape} (N, T, D)\")\n",
    "\n",
    "# %% ----------------\n",
    "# Split by user (no leakage)\n",
    "# -------------------\n",
    "user_arr = np.array(user_list)\n",
    "u_train, u_tmp, y_train, y_tmp, idx_train, idx_tmp = train_test_split(\n",
    "    user_arr, y, np.arange(len(user_arr)),\n",
    "    test_size=VAL_SIZE + TEST_SIZE, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "rel_test = TEST_SIZE / (VAL_SIZE + TEST_SIZE)\n",
    "u_val, u_test, y_val, y_test, idx_val, idx_test = train_test_split(\n",
    "    u_tmp, y_tmp, idx_tmp, test_size=rel_test, random_state=RANDOM_SEED, stratify=y_tmp\n",
    ")\n",
    "\n",
    "X_train, X_val, X_test = X[idx_train], X[idx_val], X[idx_test]\n",
    "len_train, len_val, len_test = lengths[idx_train], lengths[idx_val], lengths[idx_test]\n",
    "\n",
    "print(\"Split shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "# %% ----------------\n",
    "# Build LSTM model with masking\n",
    "# -------------------\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "inp = layers.Input(shape=(MAX_STEPS_PER_USER, ROW_FEAT_DIM), name=\"seq\")\n",
    "x = layers.Masking(mask_value=0.0)(inp)\n",
    "x = layers.LSTM(LSTM_UNITS, return_sequences=False)(x)\n",
    "x = layers.Dropout(DROPOUT)(x)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dropout(DROPOUT)(x)\n",
    "out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = models.Model(inp, out)\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[tf.keras.metrics.AUC(name=\"AUC\")]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# %% ----------------\n",
    "# Callbacks & class weights\n",
    "# -------------------\n",
    "cbs = [\n",
    "    callbacks.EarlyStopping(monitor=\"val_AUC\", mode=\"max\", patience=3, restore_best_weights=True),\n",
    "    callbacks.ReduceLROnPlateau(monitor=\"val_AUC\", mode=\"max\", factor=0.5, patience=2, min_lr=1e-5)\n",
    "]\n",
    "class_weight = {0: 1.0, 1: POS_CLASS_WEIGHT}\n",
    "\n",
    "# %% ----------------\n",
    "# Train\n",
    "# -------------------\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_weight=class_weight,\n",
    "    callbacks=cbs,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# %% ----------------\n",
    "# Evaluate\n",
    "# -------------------\n",
    "pred_val  = model.predict(X_val, batch_size=BATCH_SIZE).ravel()\n",
    "pred_test = model.predict(X_test, batch_size=BATCH_SIZE).ravel()\n",
    "\n",
    "print(f\"Validation AUC: {roc_auc_score(y_val, pred_val):.4f}\")\n",
    "print(f\"Test AUC:       {roc_auc_score(y_test, pred_test):.4f}\")\n",
    "\n",
    "# Choose threshold by maximizing Youdenâ€™s J on validation\n",
    "ths = np.linspace(0.05, 0.95, 19)\n",
    "j_scores = []\n",
    "for t in ths:\n",
    "    yhat = (pred_val >= t).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_val, yhat).ravel()\n",
    "    sens = tp / (tp + fn + 1e-9)\n",
    "    spec = tn / (tn + fp + 1e-9)\n",
    "    j_scores.append(sens + spec - 1)\n",
    "best_thr = ths[int(np.argmax(j_scores))]\n",
    "print(f\"Chosen threshold (Youden J on val): {best_thr:.2f}\")\n",
    "\n",
    "print(\"\\nTest classification report:\")\n",
    "print(classification_report(y_test, (pred_test >= best_thr).astype(int)))\n",
    "print(\"Confusion matrix (test):\")\n",
    "print(confusion_matrix(y_test, (pred_test >= best_thr).astype(int)))\n",
    "\n",
    "# %% ----------------\n",
    "# Save model & encoder (optional)\n",
    "# -------------------\n",
    "# model.save(\"user_activity_lstm.keras\")\n",
    "# import pickle\n",
    "# with open(\"ohe_province_type_creatorType.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(ohe, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music-thesis-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
